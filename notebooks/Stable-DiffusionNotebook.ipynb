{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4130ed29-0f46-470e-84f2-86e9eac7d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "from transformers import AutoFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33982b16-9ae3-4daa-84b7-e914c693a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def numpy_to_pil(images):\n",
    "    \"\"\"\n",
    "    Convert a numpy image or a batch of images to a PIL image.\n",
    "    \"\"\"\n",
    "    if images.ndim == 3:\n",
    "        images = images[None, ...]\n",
    "    images = (images * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "    return pil_images\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_replacement(x):\n",
    "    try:\n",
    "        hwc = x.shape\n",
    "        y = Image.open(\"assets/rick.jpeg\").convert(\"RGB\").resize((hwc[1], hwc[0]))\n",
    "        y = (np.array(y)/255.0).astype(x.dtype)\n",
    "        assert y.shape == x.shape\n",
    "        return y\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "\n",
    "def check_safety(x_image):\n",
    "    return x_image, [False,] * x_image.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a2326-f8d5-49d7-8d11-bdce864ca8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    root_dir: Path = Path.cwd().parent\n",
    "    output_dir: str = \"outputs/txt2img-samples-test\"\n",
    "    skip_grid: bool = False\n",
    "    skip_save: bool = True\n",
    "    ddim_steps: int = 50\n",
    "    plms: bool = True\n",
    "    laion400m: bool = False\n",
    "    ddim_eta: float = 0.\n",
    "    n_iter: int = 2\n",
    "    n_samples: int = 2\n",
    "    W: int = 512\n",
    "    H: int = 512\n",
    "    C: int = 4\n",
    "    f: int = 8\n",
    "    n_rows: int = 3\n",
    "    scale: float = 7.5\n",
    "    config: str = \"configs/stable-diffusion/v1-inference.yaml\"\n",
    "    ckpt: str = \"models/ldm/stable-diffusion-v1/model.ckpt\"\n",
    "    seed: int = 42\n",
    "    precision: str = \"autocast\"\n",
    "    fixed_code: bool = False\n",
    "    show_images: bool = True\n",
    "    \n",
    "    def prepare_config(self):\n",
    "        self.output_dir = self.root_dir / self.output_dir\n",
    "        self.config = self.root_dir / self.config\n",
    "        self.ckpt = self.root_dir / self.ckpt\n",
    "        \n",
    "config = Config()\n",
    "    \n",
    "if config.laion400m:\n",
    "    print(\"Falling back to LAION 400M model...\")\n",
    "    config.config = \"configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
    "    config.ckpt = \"models/ldm/text2img-large/model.ckpt\"\n",
    "    config.outdir = \"outputs/txt2img-samples-laion400m\"\n",
    "    \n",
    "config.prepare_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b34e8a-7358-4f89-a6c2-fab926f6d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set seed\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b985a2-9b22-4bca-adb9-d97b59a7c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_config = OmegaConf.load(f\"{config.config}\")\n",
    "model = load_model_from_config(model_config, f\"{config.ckpt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498fe5b-74d0-45b0-bd8c-850b161137bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device and send model to device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c88d5-a264-4420-8347-9188eec15c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.plms:\n",
    "    sampler = PLMSSampler(model)\n",
    "else:\n",
    "    sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74872f64-6b15-4b89-a018-fc4901ca50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = Path(config.output_dir)\n",
    "outpath.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8017d7f6-6c60-478b-9708-d75ca99ad62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = config.n_rows if config.n_rows > 0 else batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bcf6e3-3a59-4588-bf78-a6a80e1b4dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = outpath / \"samples\"\n",
    "sample_path.mkdir(parents=True, exist_ok=True)\n",
    "base_count = len(list(sample_path.glob(\"*.png\")))\n",
    "grid_count = len(list(outpath.glob(\"*.png\"))) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96635bf4-f0d6-4c0c-8483-fc106b2cb6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_code = None\n",
    "if config.fixed_code:\n",
    "    start_code = torch.randn(\n",
    "        [config.n_samples, config.C, config.H // config.f, config.W // config.f],\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c01bcb5-3994-41bc-9127-fa8acc9e4024",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_scope = autocast if config.precision==\"autocast\" else nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2732576-04f5-4f37-8389-6acb976e3f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "promt = \"\"\n",
    "batch_size = config.n_samples\n",
    "data = [batch_size * [promt]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b7df2-ba6e-4f2d-8979-49159a401a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time() # start time\n",
    "            all_samples = list()\n",
    "            for n in tqdm(range(config.n_iter), desc=\"Iterating by config.n_iter\"):\n",
    "                for prompts in data:\n",
    "                    uc = None\n",
    "                    if config.scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "                    shape = [config.C, config.H // config.f, config.W // config.f]\n",
    "                    samples_ddim, _ = sampler.sample(\n",
    "                        S=config.ddim_steps,\n",
    "                        conditioning=c,\n",
    "                        batch_size=config.n_samples,\n",
    "                        shape=shape,\n",
    "                        verbose=False,\n",
    "                        unconditional_guidance_scale=config.scale,\n",
    "                        unconditional_conditioning=uc,\n",
    "                        eta=config.ddim_eta,\n",
    "                        x_T=start_code\n",
    "                    )\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                    x_samples_ddim = x_samples_ddim.cpu().permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "                    x_checked_image_torch = torch.from_numpy(x_samples_ddim).permute(0, 3, 1, 2)\n",
    "\n",
    "                    \n",
    "                    for x_sample in x_checked_image_torch:\n",
    "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                        img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                        if not config.skip_save:\n",
    "                            img.save(sample_path / f\"{base_count:05}.png\")\n",
    "                        images.append(img)\n",
    "                        base_count += 1\n",
    "\n",
    "                    if not config.skip_grid:\n",
    "                        all_samples.append(x_checked_image_torch)\n",
    "\n",
    "            if not config.skip_grid:\n",
    "                    # additionally, save as grid\n",
    "                grid = torch.stack(all_samples, 0)\n",
    "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "                    # to image\n",
    "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                img = Image.fromarray(grid.astype(np.uint8))\n",
    "                img.save(outpath / f'grid-{grid_count:04}.png')\n",
    "                grid_count += 1\n",
    "\n",
    "            toc = time.time()\n",
    "if config.show_images:\n",
    "    for img in images:\n",
    "        img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
